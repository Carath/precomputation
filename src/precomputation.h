//////////////////////////////////////////////////////////////////////////////////////////
// Utility used to pre-compute values of a real-valued function, in order to speedup its
// evaluation by returning approximate values. 'sampleNumber' values are pre-computed
// from the image of the given function, in the preset interval [xMin, xMax]. Outside of
// this interval, images of the interval's boundaries are returned: for x < xMin, the result
// will be f(xMin); and for x > xMax, f(xMax). The approximation function, along with its
// SIMD counterpart, are thread-safe. To enable the SIMD approximation, this code needs to
// be compiled with the -march=native flag (along with -O2 or -O3). To use this utility, 
// only the 'precomputation.c' and 'precomputation.h' files have to be added to a project.
//////////////////////////////////////////////////////////////////////////////////////////

#ifndef PRECOMPUTATION_H
#define PRECOMPUTATION_H

#if __cplusplus
extern "C" {
#endif

#define PRECOMPUTATION_VERSION 1.5

//////////////////////////////////////////////////////////////////////////////////////////
// Settings:

// Compile time option, as to not cause any branch issue. Using interpolation yields greater
// accuracy, but is ~ 2 times slower. One may prefer using more samples than this option:
#define PRECOMP_USE_INTERPOLATION 0

// If set to 1, displays a message during compile time about the SIMD instructions set used.
#define VERBOSE_MODE 0

// Setting this to 1 disables the use of AVX intrinsics for the SIMD version of approximation,
// in order to use (if available) the SSE instructions set instead. This may be desirable if
// the code generated by the chosen compiler is slower with AVX than SSE.
#define FORCE_SSE 1


// When using the SIMD version of the approximation function, data must be aligned on boundaries
// of PRECOMP_ALIGNMENT bits. The way to do so depends on the data stack/heap location:

// Stack -> float __attribute__ ((aligned(PRECOMP_ALIGNMENT))) data[SIZE];
// Heap  -> float *data = (float*) aligned_alloc(PRECOMP_ALIGNMENT, size * sizeof(float));

// The 'aligned_alloc()' function needs the following line to be put at the file beginning:
// #define _ISOC11_SOURCE

// The SIMD block size is defined by PRECOMP_VECT_SIZE. Specifically:

// AVX version is enabled <=> PRECOMP_VECT_SIZE = 8
// SSE version is enabled <=> PRECOMP_VECT_SIZE = 4
// Neither are enabled    <=> PRECOMP_VECT_SIZE = 1

//////////////////////////////////////////////////////////////////////////////////////////
// Precomputation struct definition, constructor and destructor prototypes:

typedef float (*RealValuedFunction)(float);

typedef struct
{
	const int lastIndex;
	const float offset;
	const float invStepSize;
	const float* const precomputedValues;
	const RealValuedFunction theFunction;
} Precomputation;


// Allocates and initializes the pre-computation.
// N.B: 'sampleNumber' can have an impact on performances: the greater is it, the lower
// the chances of good cache behavior on pre-computed values. Empirically (with 6 Mb of CPU cache),
// there wasn't a noticeable penalty as long as sampleNumber <= 20000, and performances decrease
// slowly until sampleNumber > 1000000. Going further is really not worth it...
Precomputation* initPrecomputation(RealValuedFunction theFunction, int sampleNumber, float xMin, float xMax);


// Frees the given Precomputation, given by address.
void freePrecomputation(Precomputation **precomputation);

//////////////////////////////////////////////////////////////////////////////////////////
// Scalar approximation function:

// Computes an approximation of the evaluation of the wanted function by the given value, using
// pre-computed values. This assumes that precomputation is not NULL. Inlined for maximum speed!
// The following version is branchless:
static inline float approximation(const Precomputation *precomputation, float x)
{
	float t = x * precomputation -> invStepSize + precomputation -> offset;

	int lastIndex = precomputation -> lastIndex, index = (int) t;

	if (PRECOMP_USE_INTERPOLATION)
		t -= (float) index; // now 0 <= |t| < 1.

	// Branchless! The second line uses the first:
	index = index < 0 ? 0 : index;
	index = index > lastIndex ? lastIndex : index;

	if (!PRECOMP_USE_INTERPOLATION)
		return precomputation -> precomputedValues[index];

	return // assuming the array has 1 spot in excess:
		(1.f - t) * precomputation -> precomputedValues[index] + t * precomputation -> precomputedValues[index + 1];
}

//////////////////////////////////////////////////////////////////////////////////////////
// SIMD version of the approximation function. Must be compiled with the -march=native flag:

#if defined __AVX2__ && !FORCE_SSE
	#define SIMD_MESSAGE "Using AVX2"
	#define PRECOMP_ALIGNMENT 32
	#define PRECOMP_VECT_SIZE 8
	#define TYPE(s) __m256 ## s
	#define FUN(s) _mm256 ## s
#elif defined __SSE4_1__
	#define SIMD_MESSAGE "Using SSE4.1"
	#define PRECOMP_ALIGNMENT 16
	#define PRECOMP_VECT_SIZE 4
	#define TYPE(s) __m128 ## s
	#define FUN(s) _mm ## s
#else
	#define SIMD_MESSAGE "SSE instructions unavailable."
	#define PRECOMP_ALIGNMENT 1
	#define PRECOMP_VECT_SIZE 1
#endif

#if VERBOSE_MODE == 1
	#pragma message SIMD_MESSAGE
#endif


#if PRECOMP_VECT_SIZE > 1

#include <immintrin.h> // Loads all MMX, SSE and AVX versions.


#define FILL_DEST(dest, idx, offset)										\
{																			\
	dest[0] = precomputation -> precomputedValues[idx.i[0] + offset];		\
	dest[1] = precomputation -> precomputedValues[idx.i[1] + offset];		\
	dest[2] = precomputation -> precomputedValues[idx.i[2] + offset];		\
	dest[3] = precomputation -> precomputedValues[idx.i[3] + offset];		\
																			\
	if (PRECOMP_VECT_SIZE == 8)												\
	{																		\
		dest[4] = precomputation -> precomputedValues[idx.i[4] + offset];	\
		dest[5] = precomputation -> precomputedValues[idx.i[5] + offset];	\
		dest[6] = precomputation -> precomputedValues[idx.i[6] + offset];	\
		dest[7] = precomputation -> precomputedValues[idx.i[7] + offset];	\
	}																		\
}


// SIMD version of the approximation() function: values are processed by blocks of size PRECOMP_VECT_SIZE.
// 'src_array' and 'dest_array' can overlap, and must be aligned on boundaries of PRECOMP_ALIGNMENT bits.
static inline void approx_simd(const Precomputation *precomputation, float *dest_array, float *src_array)
{
	TYPE() x = FUN(_load_ps)(src_array);

	TYPE() invStepSize = FUN(_set1_ps)(precomputation -> invStepSize);
	TYPE() offset = FUN(_set1_ps)(precomputation -> offset);

	TYPE() t = FUN(_add_ps)(FUN(_mul_ps)(x, invStepSize), offset); // t = x * invStepSize + offset

	union { TYPE(i) xymm; int i[PRECOMP_VECT_SIZE]; } index;

	index.xymm = FUN(_cvttps_epi32)(t); // rounding towards zero: index = (int) t

	if (PRECOMP_USE_INTERPOLATION)
	{
		t = FUN(_sub_ps)(t, FUN(_cvtepi32_ps)(index.xymm)); // t = t - (float) index
	}

	// Puts index values in the range [0, lastIndex].
	// This requires SSE4.1 / AVX2. Compile with '-march=native':
	index.xymm = FUN(_max_epi32)(index.xymm, FUN(_set1_epi32)(0));
	index.xymm = FUN(_min_epi32)(index.xymm, FUN(_set1_epi32)(precomputation -> lastIndex));

	if (!PRECOMP_USE_INTERPOLATION)
	{
		FILL_DEST(dest_array, index, 0);
		return;
	}

	union { TYPE() xymm; float f[PRECOMP_VECT_SIZE]; } a; // precomputedValues[index]
	union { TYPE() xymm; float f[PRECOMP_VECT_SIZE]; } b; // precomputedValues[index + 1]

	FILL_DEST(a.f, index, 0);
	FILL_DEST(b.f, index, 1);

	// result = (1.f - t) * precomputedValues[index] + t * precomputedValues[index + 1]:
	TYPE() result = FUN(_add_ps)(FUN(_mul_ps)(FUN(_sub_ps)(FUN(_set1_ps)(1.f), t), a.xymm), FUN(_mul_ps)(t, b.xymm));

	FUN(_store_ps)(dest_array, result);
}

#undef FILL_DEST

#endif // PRECOMP_VECT_SIZE > 1

#undef FORCE_SSE
#undef VERBOSE_MODE
#undef SIMD_MESSAGE
#undef TYPE
#undef FUN

#if __cplusplus
}
#endif

#endif
